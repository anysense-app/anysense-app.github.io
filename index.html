<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="An iPhone-based application that integrates the iPhone's sensory suite with external multisensory inputs via Bluetooth and wired interfaces, enabling both offline data collection and online streaming to robots.">
  <meta name="keywords" content="AnySense">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AnySense: An iPhone App for Multi-sensory Data Collection and Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/anysense.jpeg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AnySense: An iPhone App for Multi-sensory Data Collection and Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://raunaqbhirangi.github.io/">Raunaq Bhirangi</a>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/michael-bian-40501b293/">Zeyu Bian</a>,</span>
            <span class="author-block">
              <a href="https://venkyp.com/">Venkatesh Pattabiraman</a>,
            </span>
            <span class="author-block">
              <a href="https://haritheja.com/">Haritheja Etukuru</a>,
            </span>
            <span class="author-block">
              <a href="https://eneserciyes.github.io/">Mehmet Enes Erciyes</a>,
            </span>
            <span class="author-block">
              <a href="https://mahis.life/">Nur Muhammad Mahi Shafiullah</a>,
            </span>
            <span class="author-block">
              <a href="https://www.lerrelpinto.com/">Lerrel Pinto</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">New York University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper TODO</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv TODO</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video TODO</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/NYU-robot-learning/AnySense"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<h2 class="subtitle has-text-centered" style="font-weight: bold; margin-bottom: 20px;">
  Overview
</h2>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay controls muted loop playsinline height="100%" style="width: 100%; display: block;">
        <source src="./static/videos/anysense_video.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            A key driver behind the recent advances in machine learning, especially in vision and language, has been the availability of abundant, ready-to-use data. Technologies like
            the Internet and the smartphone have played a pivotal role in this data explosion. Training such generalizable models for
            robotics is bottlenecked by our inability to collect diverse, high-quality data in the real world. One way of addressing this
            bottleneck is to create tools that combine scalable, intuitive data collection interfaces with cheap, accessible sensors. We show: (a) AnySense, an iPhone-based application that integrates the iPhoneâ€™s sensory suite with
            external multisensory inputs via Bluetooth and wired interfaces, enabling both offline data collection and online streaming to
            robots; (b) Using AnySense to interface with AnySkin, a versatile tactile sensor capable of multi-axis contact force measurement;
            and (c) Deploying robot policies trained from multisensory (visuotactile) inputs on a Stretch robot. AnySense will be fully
            open-sourced at RSS and opened up to the robotics community for multi-sensory data collection and learning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Two Column Video Section -->
    <div class="columns is-centered">
      <div class="column is-half">
        <div class="content">
          <h2 class="title is-3">Knob Turning Demo</h2>
          <p>A demonstration collected from the AnySense app collecting both tactile and contact microphone data (turn sound on).</p>
          <video id="handover-video" controls loop playsinline height="100%">
            <source src="./static/videos/knob_audio.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column is-half">
        <div class="content">
          <h2 class="title is-3">Water Pouring Demo</h2>
          <p>A demonstration collected from the AnySense app collecting both tactile and iPhone microphone data (turn sound on).</p>
          <video id="grasping-video" controls loop playsinline height="100%">
            <source src="./static/videos/water_audio.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!-- /Two Column Video Section -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- New Video Section Above Double Column -->
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Untethered Anyskin Visualization</h2>
          <p>As a user touches the magnetized skin, the readings from the magnetometer underneath it (which measures the change in magnetic flux) are streamed via bluetooth to a computer, where they can then be visualized.</p>
          <video id="main-video" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/untethered-anyskin-demo.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!-- /New Video Section -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- New Video Section Above Double Column -->
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Object Handover</h2>
          <p>We train a slip detection model using tactile readings from the AnySkin sensor as input. Once an object is grasped, a user can tug on it, which is detected by the model. The gripper then opens to hand the object to the user.</p>
          <video id="main-video" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/handover-demo.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!-- /New Video Section -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- New Video Section Above Double Column -->
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Whiteboard Erasing Policy</h2>
          <p>We use the AnySense app alongside AnySkin to collect tactile-rich video demonstrations for the task of erasing a whiteboard. Using these demonstrations, we train a behavior cloning policy and deploy our model on the Hello Robot Stretch.</p>
          <video id="main-video" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/tactile-white-board.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!-- /New Video Section -->
  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      TODO
    </code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <a class="icon-link" href="https://github.com/NYU-robot-learning/AnySense" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
